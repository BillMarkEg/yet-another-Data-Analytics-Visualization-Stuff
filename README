# yet-another-Data-Analytics-Visualization-Stuff
## Data Foundation
#### Data can: 
1- information  -> meaningful eg. age is 27
2- raw data     -> have no meaning eg. (27) so is that an age , temp degre .. .. .. etc 

#### Benefits of Data: 
cheaper -> cost reduction & effeciency 
better -> gain new opp or build new product
smarter -> understand the market and customers so can make better decisions 

#### data formats 
structured  vs semi vs unstructured

#### what is database ?
less imp -> its storage where we save data into --- so why not just store it in a-file
most imp -> its a place where we collect data together to be (sharable-reliable-more secure - fast to retrieve and more .... )

#### data silos
its about the ** operational database** where accoundting db is **separate** from marketing and sales
and maybe **incompatible** too and each one has its **separate endpoint** to acess

so there is a concept of **datawarehouse** where all of this data is collected from all Rdbms and contain history of them for **BI**

#### schema
How our data is stored ?
eg. :id is pk |  age > 0 | name < 50 charachter

#### big data
volume, variety, velocity (growing so fast) and veracity(quality of it ).

#### data strategy
1-collecting data : what should we store now and what is better to store for the future | is there data we wanna scrape/buy/generate   
2-cleaning & preparing : deal with missing ,incomplete, duplicates  and wrong data
3-leverage & capitalize : how to gai value from it |  who can acess it | should we sell it/not  

#### data platform
GCP or AWS or cloudera ...etc
so here is how we evaluate who is better based on some feasures
1-**centralized authentification** (sso) sigle sign on so you sign in once to all systems you have acess to and not have to do it multiple times
2-**improved governence** better control,audit and logging 
3-**automated dashboards ** for KPIs
4-**data visualization** : charts, graphs and analyses
5-**data lake** : structred and unstructured (even not needed with no filteration)

################################################################################################################################################
## Big Data: The Big Picture
#### past vs now 
past -> trx data , keep what we need only , inclusive data (patent and closed source) , limited storage and limited scaling
now  -> behavioral , dont keep what is not needed (default to keep) , open source , cloud and limitless storage 

#### big data use cases
1-click web padge : a/b testing ,web ads and logs
2-IOT : most form of behavioral data not just trx
3-social media : tweets and posts
4-weather and sentiment analyses

#### data gravity on cloud
means the more data you move and replicate the more you pay $ and its hard to move/upload to cloud

#### MPP
![alt text](https://github.com/BillMarkEg/yet-another-Data-Analytics-Visualization-Stuff/blob/main/mpp.png)

so now dwh too has parrall processing like spark and big data tech so its so fast and use sql too.
some notes:
Hbase/kudu -> big data no-sql works in mpp as well but not sql

#### ETL 
old school: using onpremises machines
new school: Ipaas is to make a cluster on cloud 

**CDC **: capture changes(insert/updates) on DBs
**ELT **: 1- if distincation do processing better than staging tool(spark/datastage/..) 
      2- if we wanna separate data-movement & data processing (we do this of data is too large to move )
**Data Virtualization** : processing data in source without movement and can connect to many data sources (Locality processing & parrallel processing ) ex: **spark/presto**

#### data profiles
1-data scientist , 2-engineer  
3-Data steward position: ensuring the quality, including the metadata,logging ,governance and facilitate access to it  to business users so they can understand.
4-ML engineer : feature engineering, model deployment, monitor model , retrain incase of issues
5-chef data officer: Data strtegy - data culture - data management and protection

#### technologies 
Hadoop : got from GFS (Google file system)  and M/R BY #yahooooo providers =>cloudera 
Spark : berkeley lab  - in-memory  $data bricks and cloudera too

#### sql engines in big data
hive: first sql in hadoop By Facebook, phoenix : relation databse based on hbase , spark, presto , dwh
presto : by Facebook too  to solve hive and M/R issues , **do caching & mpp like dwh || don't have storage like spark just use data virtualization concept**
Kafka: **message processing**/storage system by linkedin/ then cofluent(optimized it to use sql) 
impala : mpp in hadoop

#### common dominant formats
apache arrow : columner format **in memory**
apache parquet : columner format **in disk** so **better compression ** and **easy partitioning** of data so **save cloud cost $**

#### data governance tools
apache atlas :categorize data (like cloudera navigator)
apache ranger :role authentification across big data technologies (like apache sentry in cloudera)

#### machine learning
big data give power and large cluster to tain data instead of just taining a sample  as-of limited processing power

#### cloudera vs hortonworks
cloudera -> sentry ,cloudera manager, spark , impala , kudu , parquet , kafka
hortonworks-> atlas , ranger , tez , nifi 

#### cloudera vs hortonworks Merged together !! 
![alt text](https://github.com/BillMarkEg/yet-another-Data-Analytics-Visualization-Stuff/blob/main/cloudera%20merge%20with%20hortonworks.png)

#### Data stragegy 
data is growing so fast || data in realtime like iot/sensors =>  go to big data and stream processing tools
tools : visuals/code
################################################################################################################################################
## Enterprise data management
this course consists of 3 parts

### 1-Enterprise data strategy

#### data governance
1- make data dictionary : describe the metadata ex (table describtion,dtypes,project,example)
2-naming convention:abbreviation | _ or camelcase 
3-security mangement : to have some1 from security team to know how to secure data and use encryption keys
4-make corporate grossary : defined company terminologies ex(edge,mass,flex..etc)
5-change management : document/approve/decline changes  

#### who do data governance:
non tech : 
1-CDO(chef data offecier) 
2-governance director (direct both subject matter experts + data steward)  
3-data steward (communicate with business)
tech :
1-subject matter experts( 1-  DBA +  2 -security team member) to do technical work  
2-data modeler 

#### tools
closed source : ER studio , ERWin
open source :ER one , open modelsphere

#### Data governance program 
1-standups 
2-make a CRs form for changers to fill
3-ask if the CR will affect other systems ?
4-ask requesters of change to attend meeting and communicate with other users

#### types of data
Master : data about entity  | ex: customer,hotel   | has low fill rate | the dimention in dimentional modeling
trx : eg.sales , check-in   | has high fill rate | fact tables in Dimetional modeling
lookup : to map something ex (id and name) , eg -> egypt | it is a dimention too in dimentional modeling
meta-data : data about data

#### Data Modeling (ERD)
- how masters entity connect together and make trx data
- hotel and customer to make checkin trx 
- metadata like hotelname to be string 

logical model :  physical model 
entity        -> table
attribute     -> column
row           -> record
relationship  -> pk/fk

forward engineering : to generate code(DDL scripts to create it) from physical model

### 2-managing and working with data
Embedded SQl : quering tables directly from ur backend-application (network-load) 
storeed procedure : is better (separate app from db) (no network load) (more data encapulation) (safe from sQl ingesions attacks)

data compliance audit:
database acess ::
1-store failed login attempts 
2-change management records : if usr needs to change anything
3-principal of least previleges : to give each user minumum previligies to get his task done

subject data ::
4-appreperiate types of data (foreg. date is date not ts or sting)
5-data retention
6-who can acess and how its secured ?

### 3-getting value from data

Data engineer:collect data 
bi :  finds patterns - what if ? (past analyses) (sql + bi tools like tabeau)
data Scientist : (sampling/hyposesis/A/b testing/predictions) 

**predictions** 
supervied algo (tain/test datasets are available): decision trees | naive bayes | regression | svm
non-supervied algo tain only datasets are available: clustering(kmeans/knn) | anomaly detection

tools for algorithms: python(scikit-learn/tensorflow) -- R(caret)
visualization :  ex:matplot lib / or use tools like tableau

################################################################################################################################################
## Manageing data in the cloud
#### phases
1-Create: get data and classify (critical/normal data for security)
2-store: where to store & how  |  encryption | tokenization(splitting) | data Masking: fake copy of data for testing | digital right management : like copywrites so prevent                                                                                                                                                       users from editing ur content
3-use : 
            - if there is modification so its in creation phases so it requires reClassification
            - most vulnerable : may include movement of data to insecure storage 
            - decryption for procession 
           
4-share 
5-archive  : data become cold
6-destroy  : data is removed from cloud

#### control examples
Control : allow/prevent actions
![alt text](https://github.com/BillMarkEg/yet-another-Data-Analytics-Visualization-Stuff/blob/main/control%20examples.png)

#### Data Classification
1-Data Type
2-Contrains 
3-ownership
4-retention
5-business constains
6-encryption

#### Data reClassification
1-Data Transformation (reCheck classification of Data) ex moving data from raw to business scehma 
2-Data Movement to another platform (reCheck classification of Data) ex moving data from test (bde)  schema to production ex  ds schema  so encryption is needed (reClassification)

#### Cloud storage architecture
**IAAS** (you ctr os and every thing on in )
1-ephemeral : exists only while VM is running
2-volume : like local hard disk (folders and files ) we can use for rdbms or OS or anything just like hard-disk
3-object/blob : used for large files (all data stored with same level with id so fast access to data )
4-raw : 
5-long term : for data archiving 

**PAAS** (you ctr app and data)
1-Relation db 
2-key value pair : like Hbase
3-Map reduce : model that process data in chunks

**SAAS** (ex google drive)
1-information storage and management : ui to acess data
2-content/file storage : use volume or object for storing files on cloud
3-CDN(content deleivery Network) : data is stored and replicated in different regions so its fast to retrieve.

#### Data replication
Strict : data is replicated on all nodes but only portion % of it is avail for read/write then rest is synced (fast/no overhead) 
eventual : data is replicated on all nodes and all is avail for read/write (extreme fast / overhead in syncing )

#### storage threads
1-Data breaches : data movement to public (like what happedn in hooliwood)
2-week identy and acess mng : week password or writing password in code and upload it to github
3-insecure api : give api to others and it contains bugs they can acess other data or attach 
4-system vulnerabilities : in os itself like windows or linux issues 
5-accound hijack : login with ur password (may get it through phishing like face(p)book app)
6-malicious insider attach : attach comes from members inside 
7-Dos : hard-drive - memory can't work due to attach 
8-Ddos : distributed dos
9-spoofing identy : normail log using ur password and uname 
10-tampering with data : modification of data you have 

#### State of data
in motion : currently being accessed or transferred from host to another (downloaded - usb move - ftb)
in rest   : data not moved : ex archived 
in use    :  is data that is currently being updated, processed, accessed and read by a system

#### encryption in data:
raw level : by cloud provider when data is loaded imported/exprted from it
storage level : 
                  1-instance : exist on volume/object/application(of database)/file storage itself
                  2-proxy : there is another machine that ctr encryption of data
                  
#### Data masking : 
1-static : just once we do it manually change the data for testing
2-dynamic : a layer exists on production between client and database to change data dynamiclly through : 
                                                                                                    1- random substitution
                                                                                                    2- algorithmetic substitution
                                                                                                    3-shuffle data together
                                                                                                    4-masking (substitution only **part** of the data)
                                                                                                    5-deletion of specific **part** of the data
#### Data anonimization :
1-direct : data that identify the user like msisdn/name
2-indirect : data that can match together to identify user indirectly
so we **encrypt** them all.

#### Data Tokenization : 
tokenization uses a token to protect the data,
whereas encryption uses a key.
tokenization **swaps** sensitive data for an irreversible, nonsensitive placeholder (token) and securely stores the original, sensitive data **outside** of its original environment, and encryption **encodes the content** of a data element where it resides with a key shared between whoever is encrypting the data and whoever need to decrypt it.

#### Data leakage protection (DLP) :
ctr what data end user can transfere/download to avoid data loss or data leakage

#### data security:
consists of 1-confidenciality : least preveligies granted for user to do his task on data
            2- availability : data avail 24/7
            3- integrity : data is the same and prevent unauthorized changes
#### Data roles
subject : the subject of data 
controller : who define why we collect data how it will be processed and by who 
processer : who work on it 
steward : who maitain data governance on data & define business rules
data custodian : who technically store and maintain data (ex: DBA)
data owner : who own the data 

#### pii (identity data that should be secured)
FullName(should be full)-adress-email-id-passport-vechleNum-DOB-birthplace-msisdn-creditCard-fingerprint-handwriting-face-loginName-passwords

#### DRM (data right Mng)
1-Acl (read/write/excute)
2-integration with mail server
3-prevent print document/screenshots/copt-paste/page watermarking 

autorization : can he acess or not , it can be : 
1-sigle factor: just password
2-multifactor : password + RSA + ensure not robot and more..

authentication : what can you do (ACL)

#### cold data
1-retention --> 2-archiving --> 3-deletion
we should **keep logs monitored** on who use data and data modification

################################################################################################################################################
## principals for data quality measures

#### ML 
<br> supervised : labels
<br> unsupervised : no labels | find relationships between data
<br> semi supervised : fiew labels as it tries to use all data
<br> reEnforcement : learn from feedback and reward system 

#### Feature Engineering
<br> imputation : nulls and missing 
<br> handling outliers
<br> binning 
<br> log transform
<br> once hot encoding
<br> grouping operations 
<br> Scaling

#### Data Quality metrics
1- ratio of data to error \n
2- ratio of nulls \n
3- data transformations errors eg.float vs decimal in RM  \n
4- amount of dark data : use profiling & data discovery to understand it \n

#### Data profiling
helps to get better understanding of data through 1-Catlan 2-Ataccama

#### Data lineage 
<br> PII  : data identify user eg. msisdn and national id
<br> PII can be sensitive : ID, msisdn, passport ,visa
<br> PII can be in-sensitive : zip code, dob, gender, race
#### Benefits of data lineage and governance
<br> detect pii
<br> security
<br> autdit log
#### ML models
<br> split data into trani and test
<br> define hyperpara model use in learning 
#### ML cross validation
<br> split data into multiple folds
<br> train model on all folds except one
<br> evaluate on last one
<br> repeat step 2 & 3
<br> average result -> output performance 
#### ML risk minimization
<br> it always tries to minimize the error/loss
#### containerize
<br> build ML as a service on cloud
#### versioning ML
<br> build multiple ML models 
<br> with different hyperpara
<br> tain all and test the result on different ML objects/containers on cloud
#### ML lifecycke
<br> collect data
<br> data preparation 
<br> data wrangling
<br> analyse data
<br> train
<br> test
<br> deploy
#### MLDT : ML metadata lib
<br> tracks metdata of model excution
<br> inf about lineage
<br> model artifacts (all parameters inputed into model)

################################################################################################################################################
## Introduction to SQL

<br> table names should be singular -> person NOT persons 
<br> keywards of sql should be uppercase
<br> select * is **bad practise** as may source add columns
<br> where is boolean condition 
<br> IN -> true if any hit
<br> NOT IN -> true if all hit
<br> IS is same as = but for NULLS , IS NOT is the same as <> for nulls
<br> full outer join = left outer union right outer 
<br> self join -> useful when table contains hierarchical eg.manger id 
<br> if table is fk for another table (ex. alter table table_x add  constrain cons_name  forien key (columnName)  refrences table_y (column) ) so cant drop table_ y

################################################################################################################################################
## SQL server :understanding database fundamentals (98)
#### DDL
<br> enable/disable trigger
<br> update stats : for optimization
#### DataTypes
<br> date
<br> datetime
<br> datetime:with more details
<br> datetime offset:aware of specific timezome
<br> time
<br> smalldatetime:without second or second fraction
#### unicode Types
<br> nchar
<br> nvarchar
<br> ntext :  while nvarchar(max) will try to store the data within the database record itself. 
#### Binary Types
<br> Binary
<br> Image
<br> **VarBinary**
#### Another Types
<br> **XML**
<br> **SqlVariant** :  variable to hold values of any data type with a maximum length of 8000 bytes plus 16 bytes that holds the data type information,
#### Notes 
<br> Query A **Except** Query B : all in A and not in B
<br> Select * into B from A : create new table called B as select * from A
<br> **Truncate** is much faster than **delete from** B 
#### Understanding Data Storage 
<br> Normainzation: 1NF => split multivar attributes into multiple lines 
<br> 2NF => remove partial dependancy : column that has the same change rate as ID eg. course-name and course ID so we move it with ID to another table and make ID as FK
<br> 3NF => remove transit dependancy 
<br> transitive dependency: eg. Book → Author_Nationality: If we know the book name, we can determine the author's nationality via the Author column.
<br> so we move book_id,book_name,author_id -> table 1 
<br> auther_id,auther_name,nationality -> table 2 
#### Pk and FK
<br> FK can reference to PK or unique column 
<br> self reference is allowed : FK reference to another column in the same table
<br> Column constraints and table constraints have the **same function** 
<br> the difference is in where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY
<br> Column-level constraints (except for check constraints) refer to only one column.
<br> we use composite pk in Manay To Many tables foreg. **Not only this case for sure.**
<br> FK can referes only to column (PK/unique) in the same db only 
#### DB adminstration
<br> authentication :
<br> 1- SQL logins : sepate login to db and separate db users
<br> 2- Windows login : SSO to both windows and databse with same users 
<br> users : dbo == sysadm 
<br> Guest : user without login => just a certificate to sign in code not user by using key 
<br> user group : make a role for group of users have the same previlieges
<br> Grant: give permissions 
<br> DENY : Denies a permission to a principal. 
<br> REVOKE : Removes a previously granted or denied permission
eg. If I grant the sales group access I can later revoke it.
However I could also deny you access, and even through you're in the sales group you'll not have access.
<br> to give user ability to give previligies ==> grant **control**
<br> to give user ability to excute procedures => grant **excute**
<br> to give user ability to acess metadata => grant **view definition**
<br> SQL Server endpoint : the point of entry into SQL Server. It is implemented as a database object that defines the ways and means in which SQL
<br> **DB-Levels-Permissions** on Endpoint/db/schema/column level
<br> **DB-build-in-roles** : owner => can drop db and do anything 
<br> security_admin : modify roles and manage permissions
<br> access admin : Manage loging to windows/db    
<br> backup operator : can backup
<br> ddl-admin : can create ddls
<br> data writer : can insert/update/delete  data in user tables  
<br> data reader : read data
<br> deny data writer : can't write 
<br> u can build mixed roles
<br> **collect Statistics** (cardinality of colums , uniqueness, count of tables . . . ) sould be updated regulary for selecting better excution plan
<br> auto update stat when 20%  or more of data 's changed
<br> index : better searching 
<br> index : can be fragmented due to delete,insert and update  
<br> control index fragmentation => rebuild index , disable index , reorganize index (note:index rebuld updates stat but index reorganize dont)
<br> database maintaince plans : has ui for automated index framentation 
<br> consistancy check : corruption happens (disk failture so i/o issues) so **DBCC Checkdb** commands to check for corruption
XXXXX <br> backup types and demo  
<br>
######################################################################################################################################
#### Intro 
<br> no sql = no relational 
<br> null not stored
<br> distributed/redundant architecture -> not single point of failure 
<br> C(different statue across nodes?)A(retrieve speed)P(Data not at same place together so can recover)
<br> Relational : C + p 
<br> nosql Db   : A + P
<br> Buffer & Flush into memory (volunerable to loss (but good/less I/O) )
<br> only PK can be indexed 
<br> Query :MPP (M/R) | Like program not sql
<br> Microsoft : Azure/haddop-on-azure/HBase
<br> AWS : simpleDB/DynamoDB/Elastic M/R
<br> MongoHQ :mongoDB on cloud
<br> ALL **IAAS**
<br> #### NOSQL TYpes
<br> Key-value : shema-FREE(still rows with different schema and may different key type as well)
<br> Document : shema-FREE + json format instead of records || good for web data and personlization Eg.MongoDB
<br> Wide Column store : semi-Shema + column families  Eg.HBASE 
<br> Graph DB : relationship focused (relationships between users not their data)
<br> #### KeyValue stores 
<br> There is database & row_id to join tables together 
<br> #### wide column stores 
<br> No database but super tables & there is super columns that have sub columns (ex name : first,second )
<br> #### Document
<br> Document has Databases(which is table in normal naming)
<br> Document can has subdocument as data type (like sub json )
<br> Document can refer to another doclument by storing its document id in different column(key in json here)
<br> #### Graoh 
<br> Graph has :
<br> Nodes 
<br> Features of each Node 
<br> Edges with another nodes Eg. ahmed has (name ,age ) Edge(frient-relationshio with #ESlam) Edge(placed-relationshio with #order371)
<br> #### Relational is better if
<br> high scans
<br> trx data ex financial, debit/credit, buying & selling and so on
<br> consistancity is imp
<br> Bi reports and aggregations , joins 
<br> Enterprise OLTP
<br> #### No sql is better if
<br> low scans
<br> extreme speed is required
<br> Web corperate (web data, logs and unstructured data)
<br> No index is okay
<br> can fit in BI piplines but not reports (as its not sql/no schema so not good for bi ,complex queries and adhoc ones)
<br> #### cloud option
<br> if size of datais large => more money
######################################################################################################################################
## Recognize the need for Document Database 
#### Trx processing Vs Analytical procession
<br> Ensure **inserts** are correct (little rows)
<br> **acess** recent data 
<br> *updates*  
<br> Fast Acess 
<br> single sources (database)
#### Analytical 
<br> Large batches
<br> historical processing 
<br> Long running jobs not realtime
<br> Multiple Data sources 
#### Note
<br> NOsql Fast writes -> no need for check consistancity after each write
<br> Document db is considered as key/value nosql too                                                       
<br> Nosql -> low scan vs high scan -> Hdfs(big data lake)
#### CAP
<br> C -> every node recieve same read/write or Error
<br> A -> Every request return with no Errors
<br> P -> network failure between nodes will be auto recovered
<br> so we choose only 2 Eg. when Partition is failed in node so 2-options 
<br> 1:: to show Error -> consistencity option
<br> 2:: to return it -> availability option 
(pic19) ### N0SQL -> BASE  
<br> Basicaly Available -> replication/sharding so availablity is good
<br> Soft State -> consistencity is not guaranted in read  
<br> Eventual consistencity -> if we wait long enough consistencity is more guaranted (after writes/dropping or any modifications)
#### Relational -> ACID 
<br> Atomic - consistencity - Isolation - Durability 
<br> slow  writes than NOSQL as of consistencity  
#### Object DataBase
<br> classes and objects like in java
<br> use ORM frameworks to make data in it into code like (Jpa or hypernate)
#### Wide column DB
<br> we can NOT drop FK col in Relational 
<br> We can NOT add column in Relational without saving prev nulls
<br> so need for Wide column sql that stores data as follows : 
(pic19)
#### Document Database vs generic K/V database
<br> Kv: Dictionary as logical unig(table in relational)
<br> Doc: called Document 
<br> Kv: access  through the Key Alone 
<br> Doc: access through any attribute
<br> Kv: Key value in flat namespace  
<br> Doc: Key value in hierarchy of documents (nested=doc refer to another doc)
<br> Kv: DataModel is simple only one index on key
<br> Doc: more complex can have many indexes
#### Document Database vs relational database
<br> rel: structured data 
<br> doc: semi structured data
<br> rel: schema forced
<br> doc: schema is  flexible 
<br> rel: Data for one entity in many tables
<br> doc: Data  for one entity in same Document
<br> rel: Data for one entity in many tables
<br> doc: Data  for one entity in same Document
<br> rel: SQl 
<br> doc: No SQL
<br> rel: Metadata is stored outside table structure
<br> doc: Metadata is stored INside Document structure
(pic19) <br> mapping chart 
<br>
<br>

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>







